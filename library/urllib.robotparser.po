# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2001-2022, Python Software Foundation
# This file is distributed under the same license as the Python package.
#
# Translators:
# Adrian Liaw <adrianliaw2000@gmail.com>, 2018
# Phil Lin <linooohon@gmail.com>, 2022
msgid ""
msgstr ""
"Project-Id-Version: Python 3.10\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-26 16:47+0000\n"
"PO-Revision-Date: 2022-01-27 13:40+0800\n"
"Last-Translator: Phil Lin <linooohon@gmail.com>\n"
"Language-Team: Chinese - TAIWAN (https://github.com/python/python-docs-zh-"
"tw)\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Generator: Poedit 3.0.1\n"

#: ../../library/urllib.robotparser.rst:2
msgid ":mod:`urllib.robotparser` ---  Parser for robots.txt"
msgstr ":mod:`urllib.robotparser` --- robots.txt 的剖析器"

#: ../../library/urllib.robotparser.rst:10
msgid "**Source code:** :source:`Lib/urllib/robotparser.py`"
msgstr "**原始碼：**\\ :source:`Lib/urllib/robotparser.py`"

#: ../../library/urllib.robotparser.rst:20
msgid ""
"This module provides a single class, :class:`RobotFileParser`, which answers "
"questions about whether or not a particular user agent can fetch a URL on "
"the web site that published the :file:`robots.txt` file.  For more details "
"on the structure of :file:`robots.txt` files, see http://www.robotstxt.org/"
"orig.html."
msgstr ""
"此模組 (module) 提供了一個單獨的類別 (class) \\ :class:`RobotFileParser`\\ ，"
"它可以知道某個特定 user agent（使用者代理）是否能在有發布 :file:`robots.txt` "
"文件的網站 fetch（擷取）特定 URL。有關 :file:`robots.txt` 文件結構的更多細"
"節，請參閱 http://www.robotstxt.org/orig.html。"

#: ../../library/urllib.robotparser.rst:28
msgid ""
"This class provides methods to read, parse and answer questions about the :"
"file:`robots.txt` file at *url*."
msgstr ""
"此類別提供了一些方法可以讀取、剖析和回答關於 *url* 上的 :file:`robots.txt` 文"
"件的問題。"

#: ../../library/urllib.robotparser.rst:33
msgid "Sets the URL referring to a :file:`robots.txt` file."
msgstr "設置指向 :file:`robots.txt` 文件的 URL。"

#: ../../library/urllib.robotparser.rst:37
msgid "Reads the :file:`robots.txt` URL and feeds it to the parser."
msgstr "讀取 :file:`robots.txt` URL 並將其輸入到剖析器。"

#: ../../library/urllib.robotparser.rst:41
msgid "Parses the lines argument."
msgstr "剖析 lines 引數。"

#: ../../library/urllib.robotparser.rst:45
msgid ""
"Returns ``True`` if the *useragent* is allowed to fetch the *url* according "
"to the rules contained in the parsed :file:`robots.txt` file."
msgstr ""
"根據從 :file:`robots.txt` 文件中剖析出的規則，如果 *useragent* 被允許 fetch "
"*url* 的話，則回傳 ``True``。"

#: ../../library/urllib.robotparser.rst:51
msgid ""
"Returns the time the ``robots.txt`` file was last fetched.  This is useful "
"for long-running web spiders that need to check for new ``robots.txt`` files "
"periodically."
msgstr ""
"回傳最近一次 fetch ``robots.txt`` 文件的時間。這適用於需要定期檢查 ``robots."
"txt`` 文件更新情況的長時間運行網頁爬蟲。"

#: ../../library/urllib.robotparser.rst:57
msgid ""
"Sets the time the ``robots.txt`` file was last fetched to the current time."
msgstr "將最近一次 fetch ``robots.txt`` 文件的時間設置為當前時間。"

#: ../../library/urllib.robotparser.rst:62
msgid ""
"Returns the value of the ``Crawl-delay`` parameter from ``robots.txt`` for "
"the *useragent* in question.  If there is no such parameter or it doesn't "
"apply to the *useragent* specified or the ``robots.txt`` entry for this "
"parameter has invalid syntax, return ``None``."
msgstr ""
"針對指定的 *useragent* 從 ``robots.txt`` 回傳 ``Crawl-delay`` 參數的值。如果"
"此參數不存在、不適用於指定的 *useragent* ，或是此參數在 ``robots.txt`` 中所指"
"的條目含有無效語法，則回傳 ``None``。"

#: ../../library/urllib.robotparser.rst:71
msgid ""
"Returns the contents of the ``Request-rate`` parameter from ``robots.txt`` "
"as a :term:`named tuple` ``RequestRate(requests, seconds)``. If there is no "
"such parameter or it doesn't apply to the *useragent* specified or the "
"``robots.txt`` entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"以 :term:`named tuple` ``RequestRate(requests, seconds)`` 的形式從 ``robots."
"txt`` 回傳 ``Request-rate`` 參數的內容。如果此參數不存在、不適用於指定的 "
"*useragent* ，或是此參數在 ``robots.txt`` 中所指的條目含有無效語法，則回傳 "
"``None``。"

#: ../../library/urllib.robotparser.rst:81
msgid ""
"Returns the contents of the ``Sitemap`` parameter from ``robots.txt`` in the "
"form of a :func:`list`. If there is no such parameter or the ``robots.txt`` "
"entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"以 :func:`list` 的形式從 ``robots.txt`` 回傳 ``Sitemap`` 參數的內容。如果此參"
"數不存在或此參數在 ``robots.txt`` 中所指的條目含有無效語法，則回傳 ``None``。"

#: ../../library/urllib.robotparser.rst:89
msgid ""
"The following example demonstrates basic use of the :class:`RobotFileParser` "
"class::"
msgstr ""
"下面的範例展示了 :class:`RobotFileParser` 類別的基本用法：\n"
"\n"
"::"
